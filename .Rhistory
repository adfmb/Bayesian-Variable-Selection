library("BRugs")
install.packages("BRugs")
install.packages("BRugs")
library(iBUGS)
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
tau ~ dgamma(1.0E-4,1.0E-4)
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20)
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind [i] <- IndA[i]-1
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
for(i in 1:n){
yest[i]~dbin(pi[i],1)
}
}"
, file="ssvs_03.txt")
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Analisis_Multivariado/Bayesian-Variable-Selection")
tabla<-read.csv("datos_falsos_para_pruebas.csv",header=TRUE)
tabla_datos<-tabla
Entidad<-"DISTRITO FEDERAL"
Concurrente=1
proporcion_entrena<-.02
vector_variables<-"Porcentaje_Sustituciones"
iteraciones_jags<-1000
calentamiento_jags<-200
modelo_jags1<-1
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inits<-function(){list(alpha=58.3,tau=1,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
View(tabla_entrena2)
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM","yest")
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
#tau ~ dgamma(1.0E-4,1.0E-4) # Esta no la ponemos porque es la precisión cuando y[i] es normal
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20) #tau_in=tauBeta #sd_bet=sdBeta
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta #TauM[1]=tau[1]; TauM[2]=tau[2]
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2] #PInd[2]=p_ind[1]; PInd[1]=p_ind[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind[i] <- IndA[i]-1     #Ind=gamma
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
for(i in 1:n){
yest[i]~dbin(pi[i],1)
}
}"
, file="ssvs_03.txt")
out2 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out2))
probs <- out2$summary$statistics
?"run.JAGS"
out3 <- run.jags("ssvs_03.txt", parameters,inits=inits, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
out3 <- run.jags("ssvs_03.txt", parameters,inits=inits, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
inits<-function(){list(alpha=58.3,tau=1,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM","yest")
out3 <- run.jags("ssvs_03.txt", parameters,inits=inits, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters,inits=list(alpha=58.3,tau=1,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n)), data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters,inits=list(alpha=58.3,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n)), data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
n
var_expl
rep(1,var_expl)
IndA
IndA=c(rep(1,var_expl))
IndA
out3 <- run.jags("ssvs_03.txt", parameter, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
initlist <- replicate(8,list(m=runif(1,-20,20)),simplify=FALSE)
initlist
inits<-function(){list(alpha=58.3,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
inits
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
inits()
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits(),
method="parallel", adapt=5000, burnin=5000)
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
#tau ~ dgamma(1.0E-4,1.0E-4) # Esta no la ponemos porque es la precisión cuando y[i] es normal
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20) #tau_in=tauBeta #sd_bet=sdBeta
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta #TauM[1]=tau[1]; TauM[2]=tau[2]
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2] #PInd[2]=p_ind[1]; PInd[1]=p_ind[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind[i] <- IndA[i]-1     #Ind=gamma
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
#for(i in 1:n){
#yest[i]~dbin(pi[i],1)
#}
}"
, file="ssvs_03.txt")
inits<-function(){list(alpha=58.3,sdBeta=.5,
IndA=c(rep(1,var_expl)))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits(),
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=1,inits=inits(),
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=1,inits=inits,
method="parallel", adapt=5000, burnin=5000)
c(rep(1,var_expl))
failed.jags()
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)))}
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=1,inits=inits,
method="parallel", adapt=5000, burnin=5000)
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
probs <- out2$summary$statistics
probs <- out3$summary$statistics
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Analisis_Multivariado/Bayesian-Variable-Selection")
tabla<-read.csv("datos_falsos_para_pruebas.csv",header=TRUE)
tabla_datos<-tabla
Entidad<-"DISTRITO FEDERAL"
Concurrente=1
proporcion_entrena<-.02
vector_variables<-"Porcentaje_Sustituciones"
iteraciones_jags<-1000
calentamiento_jags<-200
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
#-Defining inits-
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
probs <- out3$summary$statistics
View(probs)
ncov <- 20
nobs <- 60
var_beta <- .004
c <- 1000
p_inclusion <- .5
sigma_y <- 1
# generate covariates
X <- array(dim=c(nobs, ncov))
for (i in 1:ncov){
X[, i] <- rnorm(nobs, 0, 1)
}
included <- rbinom(ncov, 1, p_inclusion)
coefs <- rnorm(n=ncov,
mean=0,
sd=ifelse(included==1,
sqrt(var_beta * c),
sqrt(var_beta)
)
)
coefs <- sort(coefs)
Y <- rnorm(nobs, mean=X %*% coefs, sd=sigma_y)
require(gridExtra)
require(runjags)
require(ggmcmc)
library(coda)
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)
vars <- c("alpha", "sd_bet", "gamma", "beta", "tau_in", "sd_y")
out2 <- run.jags("ssvs.txt", vars, data=dat, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf2 <- ggs(as.mcmc.list(out2))
probs2 <- out2$summary$statistics[((3):(2+ncov)), 1]
probs2 <- out2$summary$statistics
View(probs2)
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)
vars <- c("alpha", "sd_bet", "gamma", "beta", "tau_in", "sd_y")
out2 <- run.jags("ssvs.txt", vars, data=dat, n.chains=3,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out2))
probs <- summary(out2)$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- summary$out2$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- summary$out2$statistics
dat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)
vars <- c("alpha", "sd_bet", "gamma", "beta", "tau_in", "sd_y")
outdf <- ggs(as.mcmc.list(out2))
probs <- summary$out2$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- out2$summary$statistics[((2 + ncov):(1+2*ncov)), 1]
probs <- out2$summary$statistics
library(Rcpp)
library(maps)
library(mapproj)
library(ggplot2)
library(shiny)
library(R2jags)
library(rjags)
library(psych)
library(caret)
library(bnclassify)
library(klaR)
library(rocc)
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/Modelos_Lineales_Generalizados/Proyecto_equipo")
plot_proporcion_Ausentismo<-read.csv("plot_proporcion_Ausentismo.csv",header=TRUE)
plot_cantidad_Ausentismo<-read.csv("plot_cantidad_Ausentismo.csv",header=TRUE)
plot_cantidad_Casillas<-read.csv("plot_cantidad_Casillas.csv",header=TRUE)
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/Modelos_Lineales_Generalizados/Proyecto_equipo")
tabla<-read.csv("x_4_2_Solo5entidades_yDF.csv",header=TRUE)
modelo_binomial_logit<-function(){
shiny::runApp('~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/EstadisticaComputacional/Proyecto_final_c2c_5entidades')
shiny::runApp('~/Dropbox/01_ITAM_Ciencia_de_Datos/1er_semestre/EstadisticaComputacional/Proyecto_final_c2c_5entidades')
out2$summary$statistics[((2 + ncov):(1+2*ncov)), 1]
View(probs)
data.frame(coefs, pos = 1:ncov)
X %*% coefs
labels <- rep(NA, ncov)
for (i in 1:ncov){
labels[i] <- paste("beta[", i, "]", sep="")
}
labels <- rep(NA, ncov)
labels
for (i in 1:ncov){
labels[i] <- paste("beta[", i, "]", sep="")
}
labels
xdf <- data.frame(Parameter = labels, value = 1:ncov)
View(probs)
View(probs2)
xdf
or_x<-order(abs(x))
or_x<-order(abs(coef))
or_x<-order(abs(coefs))
or_coef<-order(abs(coefs))
p2 <- ggplot(df, aes(x=abs(coefs)[or_coef], y=probs[or_coef])) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
p2
ggplot(df, aes(x=abs(coefs)[or_coef], y=probs[or_coef])) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
df <- data.frame(probs=probs[or_coef], coefs = abs(coefs)[or_coef])
df
ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
out2$summary$statistics[((3):(2+ncov)), 1]
probs2 <- out2$summary$statistics[((3):(2+ncov)), 1]
labels <- rep(NA, ncov)
for (i in 1:ncov){
labels[i] <- paste("beta[", i, "]", sep="")
}
xdf <- data.frame(Parameter = labels, value = 1:ncov)
or_coef<-order(abs(coefs))
df <- data.frame(probs=probs[or_coef], coefs = abs(coefs)[or_coef])
p2 <- ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
?rbinom
rbinom(ncov, 1, p_inclusion)
?ifelse
rnorm(nobs, mean=X %*% coefs, sd=sigma_y)
summary(out2)
out2$summary$statistics
coefs
outdf
or_coef
abs(coefs)
order(abs(coefs))
?order
cfs<-numeric(length(coefs))
for(i in 1:length(coefs)){
cfs[i]<-coefs[i]
}
cfs
abs_cfs<-numeric(length(coefs))
abs_cfs<-numeric(length(coefs))
for(i in 1:length(coefs)){
cfs[i]<-coefs[i]
abs_cfs[i]<-abs(coefs[i])
}
abs_cfs
sort(abs_cfs)
order(abs_cfs)
or1<-order(abs_cfs)
ordenados<-abs_cfs[or1]
ordenados
ordenados_proba<-probs[or_coef]
ordenados_proba
probs
probs <- out2$summary$statistics[((3):(2+ncov)), 1]
probs
or_coef
abs(coefs)[or_coef]
probs[or_coef]
df <- data.frame(probs=probs[or_coef], coefs = abs(coefs)[or_coef])
ggplot(df, aes(x=coefs, y=probs)) +
geom_point(size=5, alpha=.7) +
theme_classic() +
xlab("Absolute value of true coefficient") +
ylab("Posterior probability of non-zeroness")
cat("model{
for(i in 1:n){
y[i]~dbin(pi[i],1)
#Funcion Liga
logit(pi[i])<-alpha+x[i,]%*%beta
}
#A priori's
alpha ~ dnorm(0,1.0E-6)
#tau ~ dgamma(1.0E-4,1.0E-4) # Esta no la ponemos porque es la precisión cuando y[i] es normal
tauBeta <-pow(sdBeta,-2);   sdBeta ~ dunif(0,20) #tau_in=tauBeta #sd_bet=sdBeta
TauM[1] <- tauBeta*1000;      TauM[2] <- tauBeta #TauM[1]=tau[1]; TauM[2]=tau[2]
PInd[2] <- 20/127;      PInd[1] <- 1-PInd[2] #PInd[2]=p_ind[1]; PInd[1]=p_ind[2]
for(i in 1:var_expl){
IndA[i] ~ dcat(PInd[])
Ind[i] <- IndA[i]-1     #Ind=gamma
beta[i] ~ dnorm(0, TauM[IndA[i]])
}
#Estimaciones de Ys conocidas con
#La posterior que genera Jags
#for(i in 1:n){
#yest[i]~dbin(pi[i],1)
#}
}"
, file="ssvs_03.txt")
out2$summary$statistics[((2 + ncov):(1+2*ncov)), 1]
require(gridExtra)
require(runjags)
require(ggmcmc)
library(coda)
library(Rcpp)
library(maps)
library(mapproj)
library(ggplot2)
library(shiny)
library(R2jags)
library(rjags)
library(psych)
library(caret)
library(bnclassify)
library(klaR)
library(rocc)
setwd("~/Dropbox/01_ITAM_Ciencia_de_Datos/2do_semestre/Analisis_Multivariado/Bayesian-Variable-Selection")
tabla<-read.csv("datos_falsos_para_pruebas.csv",header=TRUE)
tabla_datos<-tabla
Entidad<-"DISTRITO FEDERAL"
Concurrente=1
proporcion_entrena<-.02
vector_variables<-"Porcentaje_Sustituciones"
iteraciones_jags<-1000
calentamiento_jags<-200
# modelo_jags1<-1
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
#-Defining data-
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
outdf <- ggs(as.mcmc.list(out3))
probs <- out3$summary$statistics[((3):(2+ncov)), 1]
ncov<-var_expl
probs <- out3$summary$statistics[((3):(2+ncov)), 1]
out3$summary$statistics
names(tabla)
names(tabla)[10:40]
vector_variables<-names(tabla)[10:40]
proporcion_entrena<-.6
vector_variables<-names(tabla)[10:40]
iteraciones_jags<-1000
calentamiento_jags<-200
tabla_entrena1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c("Ausentismo2",vector_variables)))
indicador_nacional<-0
Concurrente<-unique(tabla_datos$Concurrente1[which(tabla_datos$NOMBRE_ESTADO.x==Entidad)])
tabla_resultados1<-subset(subset(tabla_datos,NOMBRE_ESTADO.x==Entidad,select=c(vector_variables,"Ausentismo2",
"Llave.Casilla","NOMBRE_ESTADO.x","iD_ESTADO.x","ID_DISTRITO.x","SECCION","ID_CASILLA","TIPO_CASILLA","EXT_CONTIGUA")))
inTraining <- createDataPartition(tabla_entrena1$Ausentismo2, p = proporcion_entrena, list = FALSE)
tabla_entrena2 <- tabla_entrena1[ inTraining,]
n<-nrow(tabla_entrena2)
var_expl<-ncol(tabla_entrena2)-1
var_expl
data<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2)
for(i in 1:var_expl){
#i<-1
data[[i+3]]<-as.array(tabla_entrena2[,i+1])
}
for( j in 1:var_expl){
#j<-1
names(data)[j+3]<-sprintf("x%i",j)
}
x<-matrix(nrow=n,ncol=var_expl)
for( j in 1:var_expl){
x[,j]<-data[[j+3]]
}
data2<-list("n"=n,"var_expl"=var_expl,"y"=tabla_entrena2$Ausentismo2,"x"=x)
inits<-function(){list(alpha=0,sdBeta=.5,
IndA=c(rep(1,var_expl)),yest=rep(0,n))}
parameters<-c("alpha","sdBeta","Ind","beta","tauBeta","TauM")#,"yest")
out3 <- run.jags("ssvs_03.txt", parameters, data=data2, n.chains=3,inits=inits,
method="parallel", adapt=5000, burnin=5000)
